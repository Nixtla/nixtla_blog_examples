{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stop Manual Feature Engineering: Automated Time Series Features with MLforecast\n\n## Setup\n\nInstall required dependencies:\n\n```bash\npip install mlforecast pandas numpy lightgbm matplotlib\n```\n\nLet's start with a simple e-commerce demand forecasting scenario where we'll explore MLforecast's automated feature engineering capabilities."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.lag_transforms import RollingMean, ExpandingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Generate sample e-commerce sales data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(\"2023-01-01\", \"2024-12-01\", freq=\"D\")\n",
    "products = [\"product_1\", \"product_2\", \"product_3\"]\n",
    "\n",
    "data = []\n",
    "for product in products:\n",
    "    # Create realistic sales patterns with trend and seasonality\n",
    "    trend = np.linspace(100, 200, len(dates))\n",
    "    seasonal = 50 * np.sin(2 * np.pi * np.arange(len(dates)) / 7)  # Weekly pattern\n",
    "    noise = np.random.normal(0, 20, len(dates))\n",
    "    sales = np.maximum(0, trend + seasonal + noise)\n",
    "\n",
    "    product_data = pd.DataFrame({\"unique_id\": product, \"ds\": dates, \"y\": sales})\n",
    "    data.append(product_data)\n",
    "\n",
    "sales_data = pd.concat(data, ignore_index=True)\n",
    "print(f\"Dataset shape: {sales_data.shape}\")\n",
    "sales_data.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic MLforecast configuration with automated features\n",
    "fcst = MLForecast(\n",
    "    models=lgb.LGBMRegressor(verbosity=-1),\n",
    "    freq=\"D\",\n",
    "    lags=[1, 7, 14],  # Previous day, week, and two weeks\n",
    "    date_features=[\"dayofweek\", \"month\"],  # Automatic date features\n",
    ")\n",
    "\n",
    "print(\"Configured features:\")\n",
    "print(f\"Lags: {fcst.ts.lags}\")\n",
    "print(f\"Date features: {fcst.ts.date_features}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLforecast handles all this complexity automatically. The `preprocess()` method:\n",
    "\n",
    "- Reads your lag configuration (`lags=[1, 7, 14]`)\n",
    "- Creates lag columns using efficient pandas operations\n",
    "- Adds configured date features automatically\n",
    "- Filters out rows where lag values cannot be calculated"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# MLforecast automated approach\n",
    "# Lags are created automatically when preprocessing\n",
    "preprocessed_data = fcst.preprocess(sales_data)\n",
    "\n",
    "print(\"Automatically created features:\")\n",
    "print(preprocessed_data.columns.tolist())\n",
    "\n",
    "# Show lag features for one product\n",
    "product_sample = preprocessed_data[preprocessed_data[\"unique_id\"] == \"product_1\"]\n",
    "print(f\"\\nLag features for product_1 (first 5 rows):\")\n",
    "print(product_sample[[\"ds\", \"y\", \"lag1\", \"lag7\", \"lag14\"]].head(5))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Enhanced MLforecast with lag transforms\nfcst_enhanced = MLForecast(\n    models=lgb.LGBMRegressor(verbosity=-1),\n    freq=\"D\",\n    lags=[1, 7, 14],\n    lag_transforms={\n        1: [RollingMean(window_size=7)],  # 7-day rolling mean of yesterday's values\n        7: [ExpandingMean()],  # Expanding mean of weekly values\n    },\n    date_features=[\"dayofweek\", \"month\"],\n)\n\n# Process data with enhanced lag features\nenhanced_data = fcst_enhanced.preprocess(sales_data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Enhanced lag features:\")\n",
    "print(enhanced_data.columns.tolist())\n",
    "\n",
    "# Show enhanced features for one product\n",
    "enhanced_sample = enhanced_data[enhanced_data[\"unique_id\"] == \"product_1\"].head(10)\n",
    "print(f\"\\nEnhanced features for product_1 (first 5 rows):\")\n",
    "print(\n",
    "    enhanced_sample[\n",
    "        [\"ds\", \"y\", \"rolling_mean_lag1_window_size7\", \"expanding_mean_lag7\"]\n",
    "    ].head()\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize rolling vs expanding means comparison\nimport matplotlib.pyplot as plt\n\n# Get last 90 days for one product for clear visualization\nproduct_viz = sales_data[sales_data[\"unique_id\"] == \"product_1\"].tail(90).copy()\nproduct_viz[\"rolling_7\"] = product_viz[\"y\"].rolling(window=7).mean()\nproduct_viz[\"expanding\"] = product_viz[\"y\"].expanding().mean()\n\n# Create visualization\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(\n    product_viz[\"ds\"],\n    product_viz[\"y\"],\n    label=\"Original Sales\",\n    color=\"white\",\n    alpha=0.6,\n    linewidth=1,\n)\nax.plot(\n    product_viz[\"ds\"],\n    product_viz[\"rolling_7\"],\n    label=\"7-day Rolling Mean\",\n    color=\"#02FEFA\",  # cyan\n    linewidth=2,\n)\nax.plot(\n    product_viz[\"ds\"],\n    product_viz[\"expanding\"],\n    label=\"Expanding Mean\",\n    color=\"#98FE09\",  # lime\n    linewidth=2,\n)\n\nax.legend()\nax.set_title(\"Rolling Mean vs Expanding Mean: Pattern Comparison\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales Units\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "## Target Transformations - Automatic Preprocessing and Postprocessing\n\nTarget transformations improve forecasting accuracy by preprocessing the target variable. For example, differencing transforms trending sales data from [100, 110, 125, 140] into changes [+10, +15, +15], making patterns easier for models to learn.\n\nMLforecast automatically handles both directions: it applies transformations during training (raw values → differences) and reverses them during prediction (model output → original scale).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure MLforecast with target transformations\n",
    "fcst_with_transforms = MLForecast(\n",
    "    models=lgb.LGBMRegressor(verbosity=-1),\n",
    "    freq=\"D\",\n",
    "    target_transforms=[Differences([1])],  # First difference transformation\n",
    "    date_features=[\"dayofweek\", \"month\"],\n",
    ")\n",
    "\n",
    "# Preprocessing automatically applies transformations\n",
    "preprocessed_with_transforms = fcst_with_transforms.preprocess(sales_data)\n",
    "\n",
    "print(\"Features with transformations:\")\n",
    "print(preprocessed_with_transforms.columns.tolist())\n",
    "\n",
    "# Show transformation results\n",
    "sample_transformed = preprocessed_with_transforms[\n",
    "    preprocessed_with_transforms[\"unique_id\"] == \"product_1\"\n",
    "].head(10)\n",
    "\n",
    "print(f\"\\nTransformed features for product_1:\")\n",
    "sample_transformed[[\"ds\", \"y\"]].head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cross-Validation for Time Series - Proper Model Evaluation\n\nStandard cross-validation uses random data splits, creating data leakage by training on future data. MLforecast's `cross_validation()` method creates multiple training/validation splits that respect temporal order.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fit the model for cross-validation\n",
    "fcst_cv = MLForecast(\n",
    "    models=lgb.LGBMRegressor(verbosity=-1),\n",
    "    freq=\"D\",\n",
    "    lags=[7, 14],\n",
    "    lag_transforms={7: [RollingMean(window_size=14)]},\n",
    "    date_features=[\"dayofweek\"],\n",
    ")\n",
    "\n",
    "# Time series cross-validation with multiple windows\n",
    "cv_results = fcst_cv.cross_validation(\n",
    "    df=sales_data,\n",
    "    n_windows=3,  # Number of validation windows\n",
    "    h=7,  # Forecast horizon (7 days)\n",
    "    step_size=7,  # Step between windows\n",
    ")\n",
    "\n",
    "print(\"Cross-validation results shape:\", cv_results.shape)\n",
    "print(\"\\nCV results sample:\")\n",
    "print(cv_results.head(5))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Evaluate performance across windows\nfrom mlforecast.utils import PredictionIntervals\nimport numpy as np\n\n\ndef mean_absolute_error(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\n\ncv_summary = (\n    cv_results.groupby([\"unique_id\", \"cutoff\"])\n    .apply(\n        lambda x: mean_absolute_error(x[\"y\"], x[\"LGBMRegressor\"]), include_groups=False\n    )\n    .reset_index(name=\"mae\")\n)\n\nprint(f\"\\nMAE by product and validation window:\")\nprint(cv_summary.head(5))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Automated Workflow - End-to-End Pipeline\n\nNow let's put all the concepts together in a complete workflow that combines lag features, transformations, and automated model training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Complete automated MLforecast workflow\n",
    "final_fcst = MLForecast(\n",
    "    models=[\n",
    "        lgb.LGBMRegressor(verbosity=-1, random_state=42),\n",
    "    ],\n",
    "    freq=\"D\",\n",
    "    lags=[1, 7, 14, 21],  # Multiple lag periods\n",
    "    lag_transforms={\n",
    "        1: [RollingMean(window_size=7), ExpandingMean()],  # Short-term patterns\n",
    "        7: [RollingMean(window_size=14)],  # Weekly patterns\n",
    "    },\n",
    "    target_transforms=[Differences([1])],  # Handle trend\n",
    "    date_features=[\"dayofweek\", \"month\", \"quarter\"],  # Seasonal features\n",
    "    num_threads=2,  # Parallel processing\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Split data for training and testing\nsplit_date = \"2024-11-01\"\ntrain_data = sales_data[sales_data[\"ds\"] < split_date]\ntest_data = sales_data[sales_data[\"ds\"] >= split_date]\n\nprint(f\"Training data: {train_data.shape}\")\nprint(f\"Test data: {test_data.shape}\")\n\n# Fit the model (automatically creates features and trains)\nfinal_fcst.fit(train_data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Generate forecasts (automatically applies transformations and reverses them)\nforecasts = final_fcst.predict(h=30)  # 30-day forecast\n\nprint(f\"\\nForecast results:\")\nprint(forecasts.head(5))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize forecast vs actual values\nimport matplotlib.pyplot as plt\n\n# Get the last 60 days of actual data + 30 days of forecasts for one product\nviz_data = sales_data[sales_data[\"unique_id\"] == \"product_1\"].tail(60)\nforecast_data = forecasts[forecasts[\"unique_id\"] == \"product_1\"]\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Plot actual values\nax.plot(\n    viz_data[\"ds\"],\n    viz_data[\"y\"],\n    label=\"Actual Sales\",\n    color=\"white\",\n    linewidth=2,\n)\n\n# Plot forecasts\nax.plot(\n    forecast_data[\"ds\"],\n    forecast_data[\"LGBMRegressor\"],\n    label=\"MLforecast Predictions\",\n    color=\"#98FE09\",  # lime\n    linewidth=2,\n)\n\n# Add vertical line to separate historical from forecast\nsplit_line = pd.Timestamp(\"2024-11-01\")\nax.axvline(\n    x=split_line,\n    color=\"#02FEFA\",  # cyan\n    linestyle=\"--\",\n    alpha=0.7,\n    label=\"Train/Test Split\",\n)\n\nax.legend()\nax.set_title(\"MLforecast Automated Predictions vs Actual Sales\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales Units\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Show feature importance (automatically created features)\nfeature_importance = final_fcst.models_[\"LGBMRegressor\"].feature_importances_\nfeature_names = final_fcst.ts.features\n\nimportance_df = pd.DataFrame(\n    {\"feature\": feature_names, \"importance\": feature_importance}\n).sort_values(\"importance\", ascending=False)\n\nprint(f\"\\nTop 10 most important automatically created features:\")\nprint(importance_df.head(10))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)",
   "path": "/Users/khuyentran/.pyenv/versions/3.11.2/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}